{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import scipy.sparse as sp\n",
    "import torch.nn.functional as F\n",
    "from rankfm.rankfm import RankFM\n",
    "import logging\n",
    "from copy import deepcopy as dp\n",
    "from sklearn.metrics import ndcg_score\n",
    "from lightfm.datasets import fetch_movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ratings.dat', delimiter='::', header=None, \n",
    "        names=['user_id', 'movie_id', 'rating', 'timestamp'], \n",
    "        usecols=['user_id', 'movie_id', 'rating'], engine='python')\n",
    "\n",
    "movie_info = pd.read_csv('movies.dat', delimiter='::', header=None, \n",
    "        names=['movie_id', 'name', 'category'], engine='python')\n",
    "\n",
    "implicit_ratings = ratings.loc[(ratings['rating'] >= 4)]\n",
    "\n",
    "users = implicit_ratings[\"user_id\"]\n",
    "movies = implicit_ratings[\"movie_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### сделаем датасет нормальным (без пропусков айдишников)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_users = len(np.unique(users))\n",
    "l_items = len(np.unique(movies))\n",
    "\n",
    "# прямое соответсвие\n",
    "user_dict = {i:j for j, i in enumerate(np.unique(users))}\n",
    "item_dict = {i:j for j, i in enumerate(np.unique(movies))}\n",
    "# обратное соответсвие\n",
    "ruser_dict = {i:j for j, i in user_dict.items()}\n",
    "ritem_dict = {i:j for j, i in item_dict.items()}\n",
    "\n",
    "df_ = implicit_ratings.loc[:, ['user_id', 'movie_id']]\n",
    "df_.user_id = df_.user_id.apply(lambda x: user_dict[x])\n",
    "df_.movie_id = df_.movie_id.apply(lambda x: item_dict[x])\n",
    "\n",
    "users = df_.user_id.max() + 1\n",
    "movies = df_.movie_id.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u, train_i = [], []\n",
    "test_u, test_i = [], []\n",
    "\n",
    "dfnp = df_.to_numpy()\n",
    "all_item_set = set(dfnp[:, 1])\n",
    "unseens = dict()\n",
    "min_len_unseens = int(1e5)\n",
    "\n",
    "for u in range(users):\n",
    "    seen = dfnp[dfnp[:, 0] == u, 1]\n",
    "    seen_train = seen[:-1]\n",
    "    seen_set = set(seen_train)\n",
    "    unseen_set = all_item_set - seen_set\n",
    "    unseens[u] = list(unseen_set)\n",
    "    min_len_unseens = min(len(unseen_set), min_len_unseens)\n",
    "    seen_test = seen[-1:]\n",
    "    train_u.extend([u] * len(seen_train))\n",
    "    train_i.extend(seen_train)\n",
    "    \n",
    "    test_u.append(u)\n",
    "    test_i.append(seen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.column_stack((test_u, test_i))\n",
    "train_data = np.column_stack((train_u, train_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# метрики, которые мы будем считать\n",
    "def calc_metrics(model, unseens, k):\n",
    "    ndcg_scores = []\n",
    "    htr_scores = []\n",
    "    for u in tqdm(range(users), position=0, leave=False, desc='calc metrix...'):\n",
    "        try:\n",
    "            seen = test_data[u]\n",
    "            unseen = np.random.choice(unseens[u], 500, replace=False)\n",
    "            data = np.column_stack(([u] * 500, unseen))\n",
    "\n",
    "            pred = model.predict(np.vstack((seen, data)))\n",
    "            real = np.hstack(([1], [0] * 500))\n",
    "            \n",
    "            ndcg_scores.append(ndcg_score([real], [pred], k=k))\n",
    "            htr_scores.append((np.argsort(pred)[::-1][:k] == 0).sum())\n",
    "        except:\n",
    "            pass\n",
    "    print(f'NDCG_{k} = {np.mean(ndcg_scores)}, HTR_{k} = {np.mean(htr_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  обоснование датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решил взять датасет из первой домашки, потому что уже знаком с ним, на нем реализовал WARP, тем более датасет популярен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF (WARP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RankFM(factors=50, loss='warp', max_samples=100, learning_schedule='invscaling')\n",
    "model.fit(train_data, epochs=50, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для оценки качества \n",
    "\n",
    "get_similars = lambda item_id, model : [movie_info[movie_info[\"movie_id\"] == ritem_dict[x]][\"name\"].to_string() \n",
    "                                        for x in model.similar_items(item_id)]\n",
    "\n",
    "get_user_history = lambda user_id, implicit_ratings : [movie_info[movie_info[\"movie_id\"] == ritem_dict[x]][\"name\"].to_string() \n",
    "                                            for x in implicit_ratings[implicit_ratings[:, 0] == user_id, 1]]\n",
    "\n",
    "get_recommendations = lambda user_id, model : [movie_info[movie_info[\"movie_id\"] == ritem_dict[x]][\"name\"].to_string()\n",
    "                                               for x in model.recommend([user_id]).to_numpy()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3045    Toy Story 2 (1999)',\n",
       " '584    Aladdin (1992)',\n",
       " '360    Lion King, The (1994)',\n",
       " '1838    Mulan (1998)',\n",
       " \"2286    Bug's Life, A (1998)\",\n",
       " '591    Beauty and the Beast (1991)',\n",
       " \"2497    Doug's 1st Movie (1999)\",\n",
       " '33    Babe (1995)',\n",
       " '1526    Hercules (1997)',\n",
       " '735    Close Shave, A (1995)']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_similars(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3399    Hustler, The (1961)',\n",
       " '2882    Fistful of Dollars, A (1964)',\n",
       " '1196    Alien (1979)',\n",
       " '1023    Die Hard (1988)',\n",
       " '257    Star Wars: Episode IV - A New Hope (1977)',\n",
       " '1959    Saving Private Ryan (1998)',\n",
       " '476    Jurassic Park (1993)',\n",
       " '1180    Raiders of the Lost Ark (1981)',\n",
       " '1885    Rocky (1976)',\n",
       " '1081    E.T. the Extra-Terrestrial (1982)',\n",
       " '3349    Thelma & Louise (1991)',\n",
       " '3633    Mad Max (1979)',\n",
       " '2297    King Kong (1933)',\n",
       " '1366    Jaws (1975)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " '2623    Run Lola Run (Lola rennt) (1998)',\n",
       " '2878    Goldfinger (1964)']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_history(3, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1180    Raiders of the Lost Ark (1981)',\n",
       " '257    Star Wars: Episode IV - A New Hope (1977)',\n",
       " '847    Godfather, The (1972)',\n",
       " '1196    Alien (1979)',\n",
       " '1366    Jaws (1975)',\n",
       " '1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
       " '585    Terminator 2: Judgment Day (1991)',\n",
       " '2502    Matrix, The (1999)',\n",
       " '1183    Good, The Bad and The Ugly, The (1966)',\n",
       " '1023    Die Hard (1988)']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_recommendations(3, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG_11 = 0.20874677999109134, HTR_11 = 0.4058674956285215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "calc_metrics(model, unseens, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### сперва сделаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dfnp, num_neg):\n",
    "        # Rating matrix\n",
    "        self.dfnp = dfnp\n",
    "       \n",
    "        self.num_users = dfnp[:, 0].max() + 1\n",
    "        self.num_items = dfnp[:, 1].max() + 1\n",
    "        \n",
    "        self.matrix = np.zeros(shape=(self.num_users, self.num_items))\n",
    "        # заполняем матрицу\n",
    "        for u, i in tqdm(self.dfnp, position=0,leave=False):\n",
    "                self.matrix[u][i] = 1\n",
    "        \n",
    "        # Prepare negs\n",
    "        self.prepare_unseen(self.dfnp, num_neg)\n",
    "        # Training set\n",
    "        self.user_input, self.item_input, self.labels = self.get_train_instances(self.dfnp, num_neg)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_input)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data.'\n",
    "        user_input = self.user_input[index]\n",
    "        item_input = self.item_input[index]\n",
    "        label = self.labels[index]\n",
    "        return {\n",
    "            'user_input': user_input,\n",
    "            'item_input': item_input,\n",
    "            'label': label\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def prepare_unseen(self, dfnp, num_neg):\n",
    "        self.unseens = dict()\n",
    "        all_item_set = set(dfnp[:, 1])\n",
    "        for u in tqdm(set(dfnp[:, 0]), position=0, leave=False, desc='prepare unseen items...'):\n",
    "            seen = dfnp[dfnp[:, 0] == u, 1]\n",
    "            seen_set = set(seen)\n",
    "            unseen_set = all_item_set - seen_set\n",
    "            self.unseens[u] = list(unseen_set)\n",
    "        \n",
    "\n",
    "    def get_train_instances(self, dfnp, num_neg):\n",
    "        user_input, item_input, labels = [], [], []\n",
    "        for (u, i) in tqdm(dfnp, position=0, leave=False, desc='prepare dataset ...'):\n",
    "            user_input.append(u)\n",
    "            item_input.append(i)\n",
    "            labels.append(1)\n",
    "            \n",
    "            negs = np.random.choice(self.unseens[u], num_neg)\n",
    "            user_input.extend([u] * num_neg)\n",
    "            item_input.extend(negs)\n",
    "            labels.extend([0] * num_neg)           \n",
    "        return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### составим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(train_data, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### структура MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.MLP_user_embedding = nn.Embedding(num_users, embed_dim)\n",
    "        self.MLP_item_embedding = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "        self.layers = nn.Sequential(nn.Linear(2 * embed_dim, hidden_size * 2),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(hidden_size * 2, hidden_size),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(hidden_size, hidden_size),\n",
    "                                      nn.ReLU())\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, feeddict, train=False):\n",
    "        user_input = feeddict['user_input']\n",
    "        item_input = feeddict['item_input']\n",
    "        # MLP\n",
    "        MLP_user_embedding = self.MLP_user_embedding(user_input)\n",
    "        MLP_item_embedding = self.MLP_item_embedding(item_input)\n",
    "        x = torch.cat([MLP_user_embedding, MLP_item_embedding], -1)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        if train:\n",
    "            x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "    \n",
    "    def predict(self, data):\n",
    "        feeddict = dict()\n",
    "        feeddict['user_input'] = torch.LongTensor(data[:, 0])\n",
    "        feeddict['item_input'] = torch.LongTensor(data[:, 1])\n",
    "        with torch.no_grad():\n",
    "            return self.forward(feeddict, True).numpy().squeeze()\n",
    "    \n",
    "    def emb(self):\n",
    "        self.U = self.MLP_user_embedding.weight.data\n",
    "        self.I = self.MLP_item_embedding.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### структура GMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim):\n",
    "        super().__init__()\n",
    "        self.GMF_user_embedding = nn.Embedding(num_users, embed_dim)\n",
    "        self.GMF_item_embedding = nn.Embedding(num_items, embed_dim)\n",
    "        \n",
    "        self.output = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, feeddict, train=False):\n",
    "        user_input = feeddict['user_input']\n",
    "        item_input = feeddict['item_input']\n",
    "        # GMF\n",
    "        GMF_user_embedding = self.GMF_user_embedding(user_input)\n",
    "        GMF_item_embedding = self.GMF_item_embedding(item_input)\n",
    "        x = GMF_user_embedding * GMF_item_embedding\n",
    "        \n",
    "        if train:\n",
    "            x = torch.sigmoid(self.output(x))\n",
    "        return x\n",
    "    \n",
    "    def predict(self, data):\n",
    "        feeddict = dict()\n",
    "        feeddict['user_input'] = torch.LongTensor(data[:, 0])\n",
    "        feeddict['item_input'] = torch.LongTensor(data[:, 1])\n",
    "        with torch.no_grad():\n",
    "            return self.forward(feeddict, True).numpy().squeeze()\n",
    "    \n",
    "    def emb(self):\n",
    "        self.U = self.GMF_user_embedding.weight.data\n",
    "        self.I = self.GMF_item_embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    @staticmethod\n",
    "    def show_similars(model, item, num):\n",
    "        model_cos = ((model.I @ model.I[item]) / np.linalg.norm(model.I, axis=1))\n",
    "        model_samples = model_cos.argsort(descending=True)[:num + 1]\n",
    "        return [movie_info[movie_info[\"movie_id\"] == ritem_dict[x.item()]][\"name\"].to_string() \n",
    "                for x in model_samples]\n",
    "    \n",
    "    \n",
    "    def show_recomendations(self, model, user, num):\n",
    "        \n",
    "        unseen = unseens[user]\n",
    "        data =  np.column_stack(([user] * len(unseen), unseen))\n",
    "        predictions = model.predict(data)\n",
    "        args_r = np.argsort(predictions)[::-1][:num]\n",
    "        \n",
    "        return  [movie_info[movie_info[\"movie_id\"] == ritem_dict[x.item()]][\"name\"].to_string() \n",
    "                for x in args_r]\n",
    "    \n",
    "T = Tester(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### претренируем GMF и MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(df_.user_id.max() + 1, df_.movie_id.max() + 1, 32, 32)\n",
    "gmf = GMF(df_.user_id.max() + 1, df_.movie_id.max() + 1, 32)\n",
    "\n",
    "mlp_optim = torch.optim.Adam(mlp.parameters(), lr=5e-3)\n",
    "gmf_optim = torch.optim.Adam(gmf.parameters(), lr=5e-3)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:0: current loss: 0.324 (MLP), 0.352 (GMF)        \n",
      "WARNING:root:1: current loss: 0.281 (MLP), 0.292 (GMF)                       \n",
      "WARNING:root:2: current loss: 0.264 (MLP), 0.222 (GMF)                       \n",
      "WARNING:root:3: current loss: 0.134 (MLP), 0.133 (GMF)                       \n",
      "WARNING:root:4: current loss: 0.304 (MLP), 0.292 (GMF)                        \n",
      "WARNING:root:5: current loss: 0.143 (MLP), 0.115 (GMF)                       \n",
      "WARNING:root:6: current loss: 0.292 (MLP), 0.260 (GMF)                        \n",
      "WARNING:root:7: current loss: 0.299 (MLP), 0.274 (GMF)                       \n",
      "WARNING:root:8: current loss: 0.149 (MLP), 0.095 (GMF)                       \n",
      "WARNING:root:9: current loss: 0.194 (MLP), 0.171 (GMF)                        \n",
      "WARNING:root:10: current loss: 0.144 (MLP), 0.132 (GMF)                       \n",
      "WARNING:root:11: current loss: 0.193 (MLP), 0.139 (GMF)                       \n",
      "WARNING:root:12: current loss: 0.160 (MLP), 0.178 (GMF)                        \n",
      "WARNING:root:13: current loss: 0.237 (MLP), 0.201 (GMF)                        \n",
      "WARNING:root:14: current loss: 0.141 (MLP), 0.164 (GMF)                       \n",
      "WARNING:root:15: current loss: 0.260 (MLP), 0.299 (GMF)                        \n",
      "WARNING:root:16: current loss: 0.208 (MLP), 0.129 (GMF)                       \n",
      "WARNING:root:17: current loss: 0.170 (MLP), 0.140 (GMF)                       \n",
      "WARNING:root:18: current loss: 0.208 (MLP), 0.256 (GMF)                        \n",
      "WARNING:root:19: current loss: 0.206 (MLP), 0.121 (GMF)                        \n",
      "WARNING:root:20: current loss: 0.163 (MLP), 0.155 (GMF)                       \n",
      "WARNING:root:21: current loss: 0.213 (MLP), 0.177 (GMF)                        \n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "loss = np.inf\n",
    "best_loss = np.inf\n",
    "for epoch in range(22):\n",
    "    \n",
    "    # Training\n",
    "    for feed_dict in tqdm(train_loader, position=0,leave=False, desc=f'{epoch}: {loss}'):\n",
    "        for key in feed_dict:\n",
    "                feed_dict[key] = feed_dict[key].to(dtype = torch.long, device = 'cpu')  \n",
    "        \n",
    "        # Forward pass\n",
    "        mlp_labels = mlp(feed_dict, True)\n",
    "        gmf_labels = gmf(feed_dict, True)\n",
    "        \n",
    "        labels = feed_dict['label'].float().view(mlp_labels.shape)\n",
    "        mlp_loss = criterion(mlp_labels, labels)\n",
    "        gmf_loss = criterion(gmf_labels, labels)\n",
    "        loss = mlp_loss + gmf_loss\n",
    "        \n",
    "        \n",
    "        mlp_optim.zero_grad()\n",
    "        gmf_optim.zero_grad()\n",
    "        \n",
    "        mlp_loss.backward()\n",
    "        gmf_loss.backward()\n",
    "        \n",
    "        mlp_optim.step()\n",
    "        gmf_optim.step()\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            save_mlp = dp(mlp)\n",
    "            save_gmf = dp(gmf)\n",
    "        \n",
    "    logging.warning(f'{epoch}: current loss: {mlp_loss:.3f} (MLP), {gmf_loss:.3f} (GMF)')\n",
    "    \n",
    "mlp.emb()\n",
    "gmf.emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### посмотрим на симиляры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0    Toy Story (1995)',\n",
       " '3045    Toy Story 2 (1999)',\n",
       " '1180    Raiders of the Lost Ark (1981)',\n",
       " '1245    Groundhog Day (1993)',\n",
       " '584    Aladdin (1992)',\n",
       " '1179    Princess Bride, The (1987)']"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.show_similars(mlp, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0    Toy Story (1995)',\n",
       " '3045    Toy Story 2 (1999)',\n",
       " '584    Aladdin (1992)',\n",
       " '1132    Wrong Trousers, The (1993)',\n",
       " '360    Lion King, The (1994)',\n",
       " '591    Beauty and the Beast (1991)']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.show_similars(gmf, 0, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### посмотрим на рекомендации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1277    Real Genius (1985)',\n",
       " '2489    Forces of Nature (1999)',\n",
       " '955    Outlaw, The (1943)',\n",
       " '1174    Madonna: Truth or Dare (1991)',\n",
       " '583    Ghost (1990)']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.show_recomendations(gmf, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['586    Dances with Wolves (1990)',\n",
       " '1213    Stalker (1979)',\n",
       " \"3616    Prizzi's Honor (1985)\",\n",
       " '1345    Crucible, The (1996)',\n",
       " '2863    Days of Heaven (1978)']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.show_recomendations(mlp, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### посмотрим на метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG_11 = 0.15358269204087233, HTR_11 = 0.3115269956939384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "calc_metrics(mlp, unseens, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG_11 = 0.16074836660115793, HTR_11 = 0.32957933090427294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "calc_metrics(gmf, unseens, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### структура NCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, mlp, gmf, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.mlp = mlp\n",
    "        self.gmf = gmf\n",
    "    \n",
    "        self.output = nn.Linear(mlp.__dict__['_modules']['output'].in_features\n",
    "                                + gmf.__dict__['_modules']['output'].in_features, 1)\n",
    "    \n",
    "    def forward(self, feeddict):\n",
    "        user_input = feeddict['user_input']\n",
    "        item_input = feeddict['item_input']\n",
    "        \n",
    "        mlp = self.mlp(feeddict, False)\n",
    "        gmf = self.gmf(feeddict, False)\n",
    "        \n",
    "        r = torch.cat([self.alpha * gmf, (1 - self.alpha) * mlp], dim=-1)\n",
    "        return torch.sigmoid(self.output(r)) \n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self.matrix[data[:, 0], data[:, 1]].numpy()\n",
    "    \n",
    "    def predict(self, data):\n",
    "        feeddict = dict()\n",
    "        feeddict['user_input'] = torch.LongTensor(data[:, 0])\n",
    "        feeddict['item_input'] = torch.LongTensor(data[:, 1])\n",
    "        with torch.no_grad():\n",
    "            return self.forward(feeddict, True).numpy().squeeze()\n",
    "    \n",
    "    def emb(self):\n",
    "        self.I = torch.cat([self.alpha * self.gmf.I, (1 - self.alpha)*self.mlp.I], dim=-1)\n",
    "        self.U = torch.cat([self.alpha * self.gmf.U, (1 - self.alpha)*self.mlp.U], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncf = NCF(mlp, gmf, 0.5)\n",
    "ncf_optim = torch.optim.Adam(ncf.parameters(), lr=5e-3)\n",
    "ncf.emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "loss = np.inf\n",
    "best_loss = np.inf\n",
    "for epoch in range(25):\n",
    "    # Training\n",
    "    for feed_dict in tqdm(train_loader, position=0,leave=False, desc=f'{epoch}: {loss}'):\n",
    "        for key in feed_dict:\n",
    "                feed_dict[key] = feed_dict[key].to(dtype = torch.long, device = 'cpu')  \n",
    "        \n",
    "        # Forward pass\n",
    "        ncf_labels = ncf(feed_dict)\n",
    "        labels = feed_dict['label'].float().view(ncf_labels.shape)\n",
    "        loss = nn.BCELoss()(ncf_labels, labels)\n",
    "        \n",
    "        \n",
    "        ncf_optim.zero_grad()\n",
    "    \n",
    "        loss.backward()\n",
    "        \n",
    "        ncf_optim.step()\n",
    "        \n",
    "        if loss < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            save_ncf = dp(ncf)\n",
    "ncf.emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### посмотрим на метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0    Toy Story (1995)',\n",
       " '584    Aladdin (1992)',\n",
       " '3045    Toy Story 2 (1999)',\n",
       " '591    Beauty and the Beast (1991)',\n",
       " \"2286    Bug's Life, A (1998)\",\n",
       " '360    Lion King, The (1994)']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.show_similars(ncf, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['583    Ghost (1990)',\n",
       " '955    Outlaw, The (1943)',\n",
       " '1560    Hoodlum (1997)',\n",
       " '2681    Radio Days (1987)',\n",
       " '2777    Adventures of Milo and Otis, The (1986)']"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.show_recomendations(ncf, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG_11 = 0.14741163532204385, HTR_11 = 0.29927128188141766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "calc_metrics(ncf, unseens, 11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
